<!doctype html>
<html lang="en">

<head>
<meta charset="utf-8">

<title>Deep Learning Course, Chapter 1</title>

<meta name="description" content="Deep Learning Course, Ferdowsi University of Mashhad, Department of Computer Science">
<meta name="author" content="Mahmood Amintoosi">

<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

<meta name="viewport" content="width=device-width, initial-scale=1.0">

<link rel="stylesheet" href="../../css/reset.css">
<link rel="stylesheet" href="../../css/reveal.css">
<link rel="stylesheet" href="../../css/theme/serif.css" id="theme">

<!-- Theme used for syntax highlighting of code -->
<link rel="stylesheet" href="../../lib/css/monokai.css">

<!-- Printing and PDF exports -->
<script>
var link = document.createElement( 'link' );
link.rel = 'stylesheet';
link.type = 'text/css';
link.href = window.location.search.match( /print-pdf/gi ) ? '../../css/print/pdf.css' : '../../css/print/paper.css';
document.getElementsByTagName( 'head' )[0].appendChild( link );
</script>

<!--[if lt IE 9]>
<script src="../../lib/js/html5shiv.js"></script>
<![endif]-->
</head>

<body>

<div class="reveal">

<!-- Any section element inside of this container is displayed as a slide -->
<div class="slides">
<!--
<section>
	<h1>Reveal.js</h1>
	<h3>The HTML Presentation Framework</h3>
	<p>
		<small>Created by <a href="http://hakim.se">Hakim El Hattab</a> and <a href="https://github.com/hakimel/reveal.js/graphs/contributors">contributors</a></small>
	</p>
</section>
<section id="themes">
	<h2>Themes</h2>
	<p>
		reveal.js comes with a few themes built in: <br>
		<!-- Hacks to swap themes after the page has loaded. Not flexible and only intended for the reveal.js demo deck. 
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/black.css'); return false;">Black (default)</a> -
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/white.css'); return false;">White</a> -
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/league.css'); return false;">League</a> -
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/sky.css'); return false;">Sky</a> -
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/beige.css'); return false;">Beige</a> -
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/simple.css'); return false;">Simple</a> <br>
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/serif.css'); return false;">Serif</a> -
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/blood.css'); return false;">Blood</a> -
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/night.css'); return false;">Night</a> -
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/moon.css'); return false;">Moon</a> -
		<a href="#" onclick="document.getElementById('theme').setAttribute('href','../../css/theme/solarized.css'); return false;">Solarized</a>
	</p>
</section> -->
<section>
 <h1>یادگیری عمیق</h1>
 <h3>دانشگاه فردوسی مشهد</h3>
 <h4>محمود امین‌طوسی</h4>
 <h3>Deep Learning</h3>
 <h4>Mahmood Amintoosi</h4>
 <p>
  <small><a href="http://https://mamintoosi-cs.github.io/">m.amintoosi</a> @ <a href="http://um.ac.ir/">um.ac.ir</a></small>
 </p>
 <p>
  <small>پاییز ۱۴۰۲</small>
 </p>
</section>

<section>
 <h2>Source book</h2>
	Deep Learning with Python, 
	<br>
	<small>
	by: FRANÇOIS CHOLLET
	</small>
	<br>
	<img height="300" src="./images/DeepLearningWithPython-book-cover.jpg" alt="Deep Learning with Python" xstyle="border:none;box-shadow:none">
	<br>
	<small>
	<a href="https://www.manning.com/books/deep-learning-with-python-second-edition">https://www.manning.com/books/deep-learning-with-python-second-edition </a>
	<br>
	<a href="https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-1/125/"> LiveBook</a><br>
	<a href="https://github.com/fchollet/deep-learning-with-python-notebooks"> Github: Jupyter Notebooks </a>
	</small>
</section>

<section>
  <section data-background="#dddddd">
   <h2>Chapter 1</h2>
   <h3>What is deep learning?</h3>
	<img height="400" src="./images/AI-ML-DL-Fig1.1.png" alt="Deep Learning" xstyle="border:none;box-shadow:none">
  </section>

  <section>
   <h2>Applications</h2>
   <p>Google Street-View (and ReCaptchas)</p>
   <img width="500" height="300" src="./images/house-numbers_598x400.png" alt="House Numbers" xstyle="border:none;box-shadow:none">
   <p><i>
     <a href="http://arxiv.org/abs/1312.6082" target="_blank">Better</a> 
      than
     <a href="http://www.geek.com/news/googles-neutral-networks-are-now-better-than-humans-at-reading-addresses-1581653/" target="_blank">Human </a>
   </i></p>
  </section>

  <section class="future" hidden="" style="top: -20px; display: none;">
   <h2>Image Classification</h2>
   <img width="500" height="400" src="./images/ImageNet-Results_574x469.png" alt="ImageNet Results" style="border:none;box-shadow:none">
   <p><i>(now better than human level)</i></p>
  </section>

  <section class="future" hidden="" style="top: -20px; display: none;">
   <h2>Captioning Images</h2>
   <img width="667" height="419" src="./images/image-labelling-results_667x419.png" alt="Labelling Results" style="border:none;box-shadow:none">
   <p><i>Some good, some not-so-good</i></p>
  </section>

  <section class="future" hidden="" style="top: -350px; display: block;">
   <h2>Speech Recognition</h2>
   <p>Android feature since <a href="http://www.phonearena.com/news/The-secret-of-Googles-amazing-voice-recognition-revealed-it-works-like-a-brain_id39938" target="_blank">Jellybean (v4.3, 2012)</a> </p>
   <p>Trained in ~5 days on 800 machine cluster</p>
   <img width="444" height="260" src="./images/speech_444x360.png" alt="Speech Recognition" xstyle="border:none;box-shadow:none">
   <small>
   <p>Embedded in phone since Android <a href="http://googleresearch.blogspot.sg/2015/08/the-neural-networks-behind-google-voice.html" target="_blank">Lollipop (v5.0, 2014)</a></p>
   </small>
  </section>

  <section class="future" hidden="" style="top: -268px; display: block;">
   <h2>Translation</h2>
   <p>Google's <a href="http://googleresearch.blogspot.sg/2015/07/how-google-translate-squeezes-deep.html" target="_blank">Deep Models</a> are on the phone</p>
   <img width="640" height="160" src="./images/google-translate_640x160.png" alt="Google Translate" xstyle="border:none;box-shadow:none">
   <p><i>"Use your camera to translate text instantly in 26 languages"</i></p>
   <p><i>Translations for typed text in 90 languages</i></p>
  </section>

<!--  
  <section>
   <h2>Reinforcement Learning</h2>
   <p>Google's DeepMind purchase</p>
   <p>Learn to play games from the pixels alone</p>
   <img width="562" height="466" src="img/deep-mind_562x466.jpg" alt="DeepMind Atari" style="border:none;box-shadow:none">
   <p><i>Better than humans 2 hours after switching on</i></p>
  </section>
!-->

  <section class="future" hidden="" style="top: -20px; display: none;">
   <h2>Reinforcement Learning</h2>
   <p>Google DeepMind's AlphaGo</p>
   <p>Learn to play Go from (mostly) self-play</p>
   <img width="902" height="337" src="./images/AlphaGo-match5_902x337.png" alt="DeepMind AlphaGo Match 5" style="border:none;box-shadow:none">
  </section>

</section>


<section >
	<section class="" style="top: -277px; display: block;">
		<h3>Machine learning vs. Classical programming</h3>
		<h4>Machine learning: a new programming paradigm</h4>
		<img height="400" src="./images/Prog-ML-Fig1.2.png" alt="Machine learning: a new programming paradigm" xstyle="border:none;box-shadow:none">
	</section>
  <section style="top: -20px; display: none;">
   <h2>Machine learning</h2>
   <ul class="fix-spacing">
    <li>Input data points</li>
    <li>Examples of the expected output</li>
	<li>A way to measure whether the algorithm is doing a good job</li>
   </ul>
   <p class="fragment" align="left">
   A machine-learning model transforms its input data into meaningful outputs, a process that is “learned” from exposure to known examples of inputs and outputs. Therefore, the central problem in machine learning and deep learning is to meaningfully transform data.
   </p>
  </section>
  
  	<section>
	<h2> Why Computer Vision is difficult?  </h2>
	<img src="images/cat01gray.png" width="300">
	
	<p class="fragment"> How Computer see the above picture? </p>
	</section>
	<section>
	<img src="images/cat01nums.png" >
	</section>
	<section data-background-iframe="images/cat01_with_space.txt">
	</section>
	
  <section >
   <h2>The “deep” in deep learning</h2>
   <ul class="fix-spacing">
    <li>The deep in deep learning isn’t a reference to any kind of deeper understanding achieved by the approach; . </li>
    <li>it stands for this idea of successive layers of representations</li>
    <li> How many layers contribute to a model of the data is called the depth of the model.</li>
   </ul>
  </img></section>
    
</section>

<section data-transition="slide" data-background="#4d7e65" data-background-transition="zoom">
	<section >
		<h2>Deep Learning Layers</h2>
		   <img height="444" src="images/A deep neural network for digit classification-fig 1.5.png" alt="DL digit" style="border:none">
	</section>
	<section >
		<h2>Deep Learning Layers</h2>
		   <img height="444" src="images/Deep representations learned by a digit-classification model-fig 1.6.png" alt="DL digit" style="border:none;box-shadow:none">
	</section>
	<section >
		<h2>Neural Networks</h2>
		   <img height="444" src="images/Figure 1.7 A neural network is parameterized by its weights.png" alt="DL digit" style="border:none;box-shadow:none">
	</section>
	<section >
		<h2>Loss Function</h2>
		   <img height="444" src="images/Figure 1.8 A loss function measures the quality of the networks output.png" alt="Loss Function" style="border:none;box-shadow:none">
	</section>
	<section >
		<h2>Optimizer</h2>
		   <img height="444" src="images/Figure 1.9 The loss score is used as a feedback signal to adjust the weights.png" alt="Optimizer" style="border:none;box-shadow:none">
	</section>

</section>

<section>
  <section data-transition="convex">
   <h2>Deep Learning breakthroughs</h2>
   <small>
   <ul class="fix-spacing">
    <li>Near-human-level image classification</li>
    <li>Near-human-level speech recognition</li>
    <li>Near-human-level handwriting transcription</li>
	<li>Improved machine translation  </li>
	<li>Improved text-to-speech conversion  </li>
	<li>Digital assistants such as Google Now and Amazon Alexa  </li>
	<li>Near-human-level autonomous driving  </li>
	<li>Ability to answer natural-language questions  </li>
	<li>Ability to answer natural-language questions  </li>
   </ul>
   </small>
  </section>
  <section data-transition="convex" >
   <h2>Deep Learning</h2>
   <ul class="fix-spacing">
    <li>Neural Networks</li>
    <li>Multiple layers</li>
    <li>Fed with lots of Data</li>
   </ul>
  </section>
  <section data-transition="convex" data-background="./images/GeoffHinton.jpeg">
   <h2>History</h2>
   <ul class="fix-spacing">
    <li>1980+ : Lots of enthusiasm for NNs</li>
    <li>1995+ : Disillusionment = A.I. Winter (v2+)</li>
    <li>2005+ : Stepwise improvement : Depth</li>
    <li>2010+ : GPU revolution : Data</li>
   </ul>
  </section>
  <section data-transition="convex">
   <h2>Who is involved</h2>
   <ul class="fix-spacing">
    <li>Google - Hinton (Toronto)</li>
    <li>Facebook - LeCun (NYC)</li>
    <li>Universities, eg: Montreal (Bengio)</li>
    <li>Baidu - Ng (Stanford)</li>
    <li>... Apple (acquisitions), etc</li>
   </ul>
   <br>
   <img src="images/BHL2.jpeg" width="300">
  </section>
  <section data-transition="convex">
   <h3>Who is involved</h3>
	<table>
		<tbody>
			<tr>
				<td>Google </td>
				<td>Hinton (Toronto)</td>
				<td><img src="images/GeoffHinton.jpeg" width="80"></td>
			</tr>
			<tr>
				<td>Facebook</td>
				<td>LeCun (NYC)</td>
				<td><img src="images/YannLeCun.jpeg" width="80"></td>
			</tr>
			<tr>
				<td>Universities</td>
				<td>Bengio (Montreal)</td>
				<td><img src="images/YoshuaBengio.jpeg" width="80"></td>
			</tr>
			<tr>
				<td>Baidu</td>
				<td>Ng (Stanford)</td>
				<td><img src="images/AndrewNg.jpeg" width="80"></td>
			</tr>
		</tbody>
	</table>
  </section>
  <section data-background="./images/AndrewNg.jpeg">
   <h2>Andrew Ng:</h2>
   <blockquote cite="https://www.gsb.stanford.edu/insights/andrew-ng-why-ai-new-electricity">
						&ldquo;AI is the new electricity.&rdquo;
	</blockquote>
  </section>
  
</section>

<section>
  <section data-transition="convex">
   <h3>2011, Image Classification</h3>
   <img width="750" height="314" src="./images/ilsvrc1_850x314.png" alt="ImageNet Karpathy" style="border:none;box-shadow:none">
   <small>
   ImageNet challenge was difficult at the time, consisting of classifying highresolution color images into 1,000 different categories after training on 1.4 million images
   </small>
  </section>
  <section data-transition="convex">
   <h4>Deep Learning started to beat other approaches...</h4>
   <small>
   <ul class="fix-spacing">
    <li>In 2011, Dan Ciresan from IDSIA began to win academic image-classification competitions with GPU-trained deep neural networks</li>
    <li>In 2011, the top-five accuracy of the winning model, based on classical approaches to computer vision, was only 74.3%.</li>
    <li>In 2012, a team led by Alex Krizhevsky and advised by Geoffrey Hinton was able to achieve a top-five accuracy of 83.6%—a significant breakthrough</li>
	<li>By 2015, the winner reached an accuracy of 96.4%, and the classification task on ImageNet was considered to be a completely solved problem  </li>
   </ul>
   </small>
  </section>
  <section data-transition="convex">
   <h3>What makes deep learning different?</h3>
	It completely automates what used to be the most crucial step in a machine-learning workflow:
	<br>	<em> feature engineering </em>
  </section>
  <section data-transition="convex">
   <h3>Why deep learning? Why now?</h3>   
	<p>In general, three technical forces are driving advances:</p>
	<xsmall>
	<ol>
     <li >Hardware</li>
	 NVIDIA GPUs, Google TPUs
	 <span class="fragment">
     <li>Datasets and benchmarks</li>
	 Flickr, YouTube videos and Wikipedia
	 </span>
	 <span class="fragment">
     <li> Algorithmic advances</li>
	   <ul>
		<li>Better activation functions </li>
		<li>Better weight-initialization schemes</li>
		<li>Better optimization schemes</li>
	   </ul>
	 </span>  
    </ol>
	</xsmall>
  </section>
</section>

<section hidden="" class="future" style="top: -270.5px; display: none;">
 <h3>- Questions? -</h3>
 <br>
 m.amintoosi @ gmail.com
 <br>
 <p>webpage : <a href="http://mamintoosi.ir/">http://mamintoosi.ir</a></p>
 <p>webpage in github : <a href="https://mamintoosi-cs.github.io/">http://mamintoosi-cs.github.io</a></p>
 <p>github : <a href="https://github.com/mamintoosi">mamintoosi</a></p>
 <hr>
<!-- <small>
 <p> مطالب و سبک اسلاید برگرفته از 
	<a href="https://github.com/mdda/deep-learning-workshop"> Martin Andrews </a>
	و
	<a href="https://hameds.github.io/slides/"> سعیدی‌فرد </a>
 </p>
 </small>
--> 
</section>	

</div>

</div>

<script src="../../js/reveal.js"></script>

<script>

// More info https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({
	controls: true,
	progress: true,
	center: true,
	hash: true,
	slideNumber: true,

	transition: 'slide', // none/fade/slide/convex/concave/zoom

	// More info https://github.com/hakimel/reveal.js#dependencies
	dependencies: [
		{ src: '../../plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
		{ src: '../../plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
		{ src: '../../plugin/highlight/highlight.js', async: true },
		{ src: '../../plugin/search/search.js', async: true },
		{ src: '../../plugin/zoom-js/zoom.js', async: true },
		{ src: '../../plugin/notes/notes.js', async: true }
	]
});

</script>

</body>
</html>
