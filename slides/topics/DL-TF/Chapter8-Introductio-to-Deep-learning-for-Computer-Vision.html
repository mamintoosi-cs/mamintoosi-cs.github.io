<!doctype html>
<html lang="en">

<head>
	<meta charset="utf-8">

	<title>Deep Learning Course, Chapter 8</title>

	<meta name="description"
		content="Deep Learning Course, Ferdowsi University of Mashhad, Department of Computer Science">
	<meta name="author" content="Mahmood Amintoosi">

	<meta name="apple-mobile-web-app-capable" content="yes">
	<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">

	<meta name="viewport" content="width=device-width, initial-scale=1.0">

	<link rel="stylesheet" href="../../css/reset.css">
	<link rel="stylesheet" href="../../css/reveal.css">
	<link rel="stylesheet" href="../../css/theme/serif.css" id="theme">

	<!-- Theme used for syntax highlighting of code -->
	<link rel="stylesheet" href="../../lib/css/monokai.css">

	<!-- Printing and PDF exports -->
	<script>
		var link = document.createElement('link');
		link.rel = 'stylesheet';
		link.type = 'text/css';
		link.href = window.location.search.match(/print-pdf/gi) ? '../../css/print/pdf.css' : '../../css/print/paper.css';
		document.getElementsByTagName('head')[0].appendChild(link);
	</script>

	<!--[if lt IE 9]>
<script src="../../lib/js/html5shiv.js"></script>
<![endif]-->
</head>
<!-- <style>
					.fragment.visible:not(.current-fragment) {
						display: none;
						height: 0px;
						line-height: 0px;
						font-size: 0px;
					}
				</style> -->

<body>

	<div class="reveal">

		<!-- Any section element inside of this container is displayed as a slide -->
		<div class="slides">
			<section>
				<h2>یادگیری عمیق</h2>
				<h3>Deep Learning</h3>
				<h4>Chapter 8: Introduction to Deep learning for Computer Vision</h4>
				<h5>Mahmood Amintoosi</h5>
				<p>
					<small><a href="http://https://mamintoosi.github.io/">m.amintoosi</a> @ <a
							href="http://um.ac.ir/">um.ac.ir</a></small>
				</p>
				<p>
					<small>پاییز ۱۴۰۲</small>
				</p>
			</section>

			<section>
				<h2>Source book</h2>
				Deep Learning with Python,
				<br>
				<small>
					by: FRANÇOIS CHOLLET
				</small>
				<br>
				<img height="300" src="./images/DeepLearningWithPython-book-cover.jpg" alt="Deep Learning with Python"
					xstyle="border:none;box-shadow:none">
				<br>
				<small>
					<a href="https://www.manning.com/books/deep-learning-with-python-second-edition">https://www.manning.com/books/deep-learning-with-python-second-edition
					</a>
					<br>
					<a href="https://livebook.manning.com/book/deep-learning-with-python-second-edition/chapter-1/125/">
						LiveBook</a><br>
					<a href="https://github.com/fchollet/deep-learning-with-python-notebooks"> Github: Jupyter Notebooks
					</a>
				</small>
			</section>

			<section>
				<h2>Chapter 8</h2>
				<h3>Introduction to deep learning for computer vision</h3>
				<p>This chapter covers:</p>
				<ul class="fix-spacing">
					<li>Understanding convolutional neural networks
					</li>
					<li> Using data augmentation to mitigate overfitting
					</li>
					<li> Using a pretrained convnet to do feature
						extraction
					</li>
					<li> Fine-tuning a pretrained convnet
					</li>
				</ul>
			</section>

			<section>
				<section>
					<h2> Why Computer Vision is difficult? </h2>
					<img src="images/cat01gray.png" width="300">

					<p class="fragment"> How Computer see the above picture? </p>
				</section>
				<section>
					<img src="images/cat01nums.png">
				</section>
				<section data-background-iframe="images/cat01_with_space.txt">
				</section>

			</section>

			<!-- <section>
				<section>
					<h3>Some of My Papers</h3>
					<ol>
						<li><a
								href="http://deeplearning.net/software/theano_versions/0.9.X/tutorial/conv_arithmetic.html">Convolution
								arithmetic tutorial</a>
						</li>
						<li><a href="https://mlblr.com/includes/mlai/index.html#ml-amp-ai"> Machine Learning and AI -
								Bangalore Chapter</a></li>
						<li><a
								href="https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889">
								Counting No. of Parameters in Deep Learning Models by Hand</a></li>
					</ol>
				</section>
				<section>
					data-background-iframe="http://deeplearning.net/software/theano_versions/0.9.X/tutorial/conv_arithmetic.html#"
					data-background-interactive>
				</section>

				<section data-background-iframe="https://mlblr.com/includes/mlai/index.html#ml-amp-ai"
					data-background-interactive>
				</section>
			</section> -->
			<section>
				<section>
					<h3>Mathematical definition of Convolution</h3>
					<img src="images/conv.png" width="750"><br>
					Source: <a href="https://en.wikipedia.org/wiki/Convolution">Wikipedia</a>
					<small class="fragment">
						A good resource:
						<a href="http://www.inf.ed.ac.uk/teaching/courses/cfcs1/lectures/cfcs_l15.pdf">
							Computational Foundations of Cognitive Science, Lecture 15: Convolutions and Kernels, School
							of Informatics
							University of Edinburgh
						</a>
					</small>
				</section>
				<section data-background-iframe="part15.pdf" data-background-interactive>
				</section>
				<section>
					<img src="images/Example-3-1-5-Zahedi.png" width="600">
					<br>
					<small>
						<a href="http://hcloud.hsu.ac.ir/index.php/s/uWBr88tQvDvhbqe#pdfviewer">Zahedi, MSc Thesis:
							Framelet-based image inpainting and data recovery, Page 43</a>
					</small>
				</section>
				<section>
					<h3>1D Convolution</h3>
					The last plot in Chap. 4:<br>
					<img src="images/truncated_mae_history.png" width="400">
					<img src="images/truncated_mae_history-conv-1d.png" width="400" class="fragment">
					<br>
					<a href="https://colab.research.google.com/github/fum-cs/dl/blob/main/code/conv_play.ipynb">Play
						with Convs & Filters</a>
				</section>
			</section>
			<section>
				<section>
					<img src="images/Fig-2-12-DeepLearning.png" width="800">
					<small>
						Deep Learning with Python, by François Chollet, Second Edition, 2021, Fig. 2.12
					</small>
				</section>
				<section>
					<img src="images/Fig-2-12-Relu-DeepLearning.png" width="800">
					<small>
						Deep Learning with Python, by François Chollet, Second Edition, 2021
					</small>
				</section>
				<section>
					<h2> Convolution as Matrix Production</h2>
					<img src="images/Fig-3-12-Szeliski.png" width="800">
					<small>
						<a href="https://fumdrive.um.ac.ir/index.php/s/onqa8MkGCcogtY2">
							Computer Vision Algorithms and Applications, Szeliski, Second Edition, 2021, Fig. 3.12, Page
							122
						</a>
					</small>
				</section>
			</section>
			<section>
				<section data-background-iframe="https://www.mathworks.com/help/optim/ug/deblur-problem-based.html"
					data-background-interactive>
				</section>
				<section>
					<h4>Finding x in min||Ax-b||</h4>
					<a
						href="https://colab.research.google.com/github/fum-cs/dl/blob/main/code/Large-Scale-Constrained-Linear-Least-Squares.ipynb">Run
						Python code in Colab</a>
					<table>
						<tr>
							<td><img src="images/optdeblur_01-output.jpg" width="850"></td>
						</tr>
					</table>
					<small>
						In DL, A, as filter weights, is un-known when x, b are in-hand
					</small>
				</section>
			</section>
			<section>
				<section>
					<h4>Differentiation, Inner Product and Convolution</h4>	
						<!-- Convolution was a weighted averaging?<br> -->
						First derivative: <br>
						<img src="images/derivative.png" width="500">
						<br>
						h = [1,-1]
						<br>
					<small class="fragment">
					<br>
					<a href="https://www.rapidtables.com/calc/math/convolution-calculator.html">
						Check online convolution</a>:  g*h, when g = [5,8] or h*h
					</small>
					<a href="https://phiresky.github.io/convolution-demo/">Convolution demo</a>
				</section>
				<section>
					<small>
							Edge extraction is a simple derivative:<br>
							One of the edge detection operators is Prewitt filter:
							<br>
							<table style="border: 1px solid black">
								<tr>
									<td>-1</td>
									<td>0</td>
									<td>+1</td>
								</tr>
								<tr>
									<td>-1</td>
									<td>0</td>
									<td>+1</td>
								</tr>
								<tr>
									<td>-1</td>
									<td>0</td>
									<td>+1</td>
								</tr>
							</table>
					</small>
					<img src="images/derivative_Prewitt.png" width="500" class="fragment">
				</section>
				<!-- An n-D convolution is when two functions or tensors are convolved along [math]n[/math] axes.
In an RGB image, the image is [math]a*b*3[/math] and the convolutional filters are [math]c[/math][math]*d*3[/math]. Since their third dimensions are equal, there is no need to convolve along that axis. 
You only convolve along the first two axes, making it a 2D convolution.
Convolutions were originally defined for functions and functions can have vector outputs.
 In the case of an RGB image, the image is acting as the function, its input is the [math](x,y)[/math] coordinates and its output is a vector of length 3. The same is true for the filter. When you think of it that way, 
you can see how it follows the definition of a 2D convolution more clearly. -->
				<section>
					<h3>Sobel Filter</h3>
					<small align="left">
						If we define A as the source image, and Gx and Gy are two images which at each point contain the
						horizontal and vertical derivative approximations respectively, the computations are as follows
						(<a href="https://en.wikipedia.org/wiki/Sobel_operator">source wiki</a>):
					</small>
					<img src="images/Sobel_01.png" width="500">
					<p class="fragment">
						<img src="images/convSobel.gif" width="350">
					</p>
				</section>
				<section>
					<small> Online
						<a href="https://fiveko.com/online-tools/edge-detection-gradient-operators-demo/">
							Edge Detection: Gradient operators Demo
						</a><br>
						See <a href="https://github.com/mamintoosi-cs/pytorch-workshop/App_Image.ipynb"> App_Image.ipynb
						</a>
						or Run in <a
							href="https://colab.research.google.com/github/mamintoosi-cs/pytorch-workshop/blob/master/App_Image.ipynb">
							Colab </a>
						<br>
					</small>
					<table>
						<tr>
							<td></td>
							<td><img src="images/Soleymani_mellat.jpg" width="450"></td>
							<td></td>
						</tr>
						<tr>
							<td><img src="images/Soleymani_mellat_V.jpg" width="450"></td>
							<td><img src="images/Soleymani_mellat_H.jpg" width="450"></td>
							<td><img src="images/Soleymani_mellat_generic.jpg" width="450"></td>
						</tr>
					</table>
				</section>
			</section>
			<section>
				<section>
					<h3>Laplacian</h3>
					<small align="left">
						The Laplace operator is a second-order differential operator in the n-dimensional Euclidean
						space, defined as the divergence ($\nabla \cdot$) of the gradient ($\nabla f$). Thus if $f$ is a
						twice-differentiable real-valued function, then the Laplacian of $f$ is the real-valued function
						defined by:<br>
						$\Delta f=\nabla ^{2}f=\nabla \cdot \nabla f$<br>
					</small>
					<br>
					<img src="images/first_and_second_derivative.webp" width="550">
				</section>
				<section>
					<h3>Laplacian</h3>
					<small align="left">
						As we saw [-1,0,1] demonstrate Prewitt filter weights, which is a first order derivative;
						Convolving [-1,0,1] with your image basically computes the difference between the pixel values
						of the neighboring pixels. You apply 0 to the current pixel, 1 to the pixel on the right and -1
						to the pixel on the left. This gives a first order difference:
						<center>
							next pixel - previous pixel
						</center>
						<br><br>
						The Laplacian operator looks something like [1, -2, 1]. This computes the difference of
						differences. To see how, note that [1,-2,1] corresponds to:<br>
						<center>
							next - 2 x current + previous
						</center>
						<br>
						<center>
							next - current - current + previous
							<br>
							(next-current) - (current-previous)
						</center>
						<br>
						Now notice how this is a diference of differences. (next - current) is like a 1st derivative.
						(current - previous) is like 1st derivative. Their difference is like a 2nd derivative.
					</small>
				</section>
				<section>
					<img src="images/laplacian_01.png" width="500"><br>
					Adding these two kernels:<br>
					<img src="images/laplacian_02.png" width="500">
				</section>
				<section>
					<h3>Laplacian</h3>
					<small>
						See <a
							href="https://colab.research.google.com/github/mamintoosi-cs/pytorch-workshop/blob/master/App_Image.ipynb">
							App_Image.ipynb </a>
					</small><br>
					<table>
						<tr>
							<td><img src="images/Soleymani_mellat.jpg" width="450"></td>
							<td><img src="images/Soleymani_mellat_LoG.png" width="450"></td>
						</tr>
					</table>
				</section>

			</section>
			<section>
				<section data-background-iframe="https://connectjaya.com/classical-convolutional-neural-networkscnn/"
					data-background-interactive>
				</section>

			</section>
			<section>
				<section>
					<h3>Classification with CNNs</h3>
					<ol>
						<li>English Digit Classification</li>
						<li>Persian Digit Classification</li>
						<li>Classifying Cats vs Dogs</li>
					</ol>
				</section>
				<section>
					<code>8.1 - Introduction to convnets</code>
					<br>
					MNIST Classification (Included with Keras)
					<br> Overall Model:
					<br>
					<img src="images/digitCNN.jpeg">
				</section>
				<section>
					<code>8.1 - Introduction to convnets</code>
					<br>
					MNIST Classification, Sequential Model
					<pre>
		 <code class="hljs" data-trim>
from tensorflow import keras
from tensorflow.keras import layers

model = keras.Sequential()
model.add(layers.Conv2D(32, (5, 5), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (5, 5), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
				 </code>
		 </pre>
				</section>
				<section>
					<h3>Number of Parameters of Sequential Model</h3>
					<pre>
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
conv2d_6 (Conv2D)           (None, 24, 24, 32)        832       																	
max_pooling2d_2 (MaxPoolin  (None, 12, 12, 32)        0         
g2D)                                                            																
conv2d_7 (Conv2D)           (None, 8, 8, 64)          51264     																
max_pooling2d_3 (MaxPoolin  (None, 4, 4, 64)          0         
g2D)                                                            																
flatten_2 (Flatten)         (None, 1024)              0         																
dense_2 (Dense)             (None, 64)                65600     																
dense_3 (Dense)             (None, 10)                650       																
=================================================================
Total params: 118346 (462.29 KB)
Trainable params: 118346 (462.29 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
		 </pre>
				</section>
			</section>
			<!-- <section>
				<section>
					<section
						data-background-iframe="https://connectjaya.com/classical-convolutional-neural-networkscnn/"
						data-background-interactive>
					</section>
					<xsmall>
						<a
							href="https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53">
							Overall Architecture of a sample CNN</a>
					</xsmall><br>
					<img src="images/cnn_overall.jpeg">
					<small>
						More about architecture and number of parameters:
						<ul>
							<li><a
									href="https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889">
									Counting No. of Parameters in Deep Learning Models by Hand </a> </li>
							<li>
								<a
									href="https://medium.com/@iamvarman/how-to-calculate-the-number-of-parameters-in-the-cnn-5bd55364d7ca">
									How to calculate the number of parameters in the CNN? </a>
							</li>
Convolutional Layer : Consider a convolutional layer which takes “l” feature maps as the input and has “k” feature maps as output. The filter size is “n*m”.
Here the input has l=32 feature maps as inputs, k=64 feature maps as outputs and filter size is n=3 and m=3. It is important to understand, that we don’t simply have a 3*3 filter, but actually, we have 3*3*32 filter, as our input has 32 dimensions. And as an output from first conv layer, we learn 64 different 3*3*32 filters which total weights is “n*m*k*l”. Then there is a term called bias for each feature map. So, the total number of parameters are “(n*m*l+1)*k”.			
						</ul>
					</small>
				</section>
				<section>
					<img src="images/cnn_1_3.png" width="700">
					<small>Source:<a
							href="https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889">
							Counting No. of Parameters in Deep Learning Models by Hand</a></small>
				</section>
				<section>
					<img src="images/cnn_3_1.png" width="700">
					<small>Source:<a
							href="https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889">
							Counting No. of Parameters in Deep Learning Models by Hand</a></small>
				</section>
				<section>
					<img src="images/cnn_2_3.png" width="500">
					<small>Source:<a
							href="https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889">
							Counting No. of Parameters in Deep Learning Models by Hand</a></small>
				</section>
			</section> -->
			<section>
				<section>
					<code>8.1 - Introduction to convnets</code>
					<br>
					MNIST Classification, API Model
					<pre>
		 <code class="hljs" data-trim>
from tensorflow import keras
from tensorflow.keras import layers
inputs = keras.Input(shape=(28, 28, 1))
x = layers.Conv2D(filters=32, kernel_size=5, activation="relu")(inputs)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Conv2D(filters=64, kernel_size=5, activation="relu")(x)
x = layers.MaxPooling2D(pool_size=2)(x)
x = layers.Flatten()(x)
x = layers.Dense(64, activation="relu")(x)
outputs = layers.Dense(10, activation="softmax")(x)
model = keras.Model(inputs=inputs, outputs=outputs)
				 </code>
		 </pre>
				</section>
				<section>
					<h3>Number of Parameters of API Model</h3>
					<pre>
_________________________________________________________________
Layer (type)                Output Shape              Param #   
=================================================================
input_4 (InputLayer)        [(None, 28, 28, 1)]       0         														
conv2d_10 (Conv2D)          (None, 24, 24, 32)        832       
max_pooling2d_6 (MaxPoolin  (None, 12, 12, 32)        0         
g2D)                                                            
conv2d_11 (Conv2D)          (None, 8, 8, 64)          51264     
max_pooling2d_7 (MaxPoolin  (None, 4, 4, 64)          0         
g2D)                                                            
flatten_4 (Flatten)         (None, 1024)              0         
dense_6 (Dense)             (None, 64)                65600     
dense_7 (Dense)             (None, 10)                650       
=================================================================
Total params: 118346 (462.29 KB)
Trainable params: 118346 (462.29 KB)
Non-trainable params: 0 (0.00 Byte)
_________________________________________________________________
</pre>
				</section>
				<section>
					<code> -- </code>
					<br>
					Persian Digits Classification (Not included with Keras)
					<pre>
		 <code class="hljs" data-trim>
import keras
from keras import layers
from keras import models

model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(64, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))	
		 </code>
		 </pre>
				</section>
			</section>
			<section>
				<section>
					<code>8.2 - Training a convnet from scratch on a small dataset</code>
					<br>
					Classify Dogs vs Cats (Not included with Keras)
					<br>
					<img src="images/Fig5.8-DogsCats.jpg">
					<img src="images/image-classification-cat-1.jpg" class="fragment">
				</section>
				<section>
					<code>8.2 - Training a convnet from scratch on a small dataset</code>
					<br>
					Classify Dogs vs Cats (Building from scrach)
					<ul>
						<li>Download Images from <a href="http://www.kaggle.com/c/dogs-vs-cats/data"> Kaggle </a>
						</li>
						<li>
							Download Images from <a
								href="https://www.floydhub.com/api/v1/resources/MmV389WwjuivQhKXoA9GaR?content=true&download=true&rename=fastai-datasets-cats-vs-dogs-2">
								fastai </a> 845MB, (need some manipulation)
						</li>
					</ul>
				</section>
				<section>
					<code>8.2 - Training a convnet from scratch on a small dataset</code>
					<br>
					Classify Dogs vs Cats (Building from scrach)
					<pre>
		 <code class="hljs" data-trim>
model = models.Sequential()
model.add(layers.Conv2D(32, (3, 3), activation='relu',
                        input_shape=(150, 150, 3)))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(64, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Conv2D(128, (3, 3), activation='relu'))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Flatten())
model.add(layers.Dense(512, activation='relu'))
model.add(layers.Dense(1, activation='sigmoid'))			
		 </code>
		 </pre>
				</section>
			</section>

			<section>
				<section>
					There are various architectures of CNNs available<br>
					LeNet, AlexNet, VGGNet, GoogLeNet, ResNet, ZFNet <br>
					<img src="images/acc_vs_net_vs_ops.svg">
				</section>
				<section>
					<a href="https://neurohive.io/en/popular-networks/vgg16/">
						VGG16 Architecture </a><br>
					<img src="images/vgg16.png">
				</section>
				<section data-background="images/nvidia-gtx-titan-black.jpg" data-background-opacity=.2>
					VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the
					University of Oxford in the paper “Very Deep Convolutional Networks for Large-Scale Image
					Recognition”. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over
					14 million images belonging to 1000 classes. VGG16 was trained for weeks and was using NVIDIA Titan
					Black GPU’s.
				</section>
			</section>

			<section>
				<section>
					ML vs DL <br>
					<img src="images/MLvsDL_ces2016.png">
				</section>
				<section>
					Some Outputs <br>
					<img src="images/out_01.jpg">
					<img src="images/yolo_pred.jpg">
				</section>
				<section>
					Our Outputs <br>
					<img src="images/1_2 (9).jpg">
					<img src="images/1_5 (12).jpg">
				</section>
				<section>
					Our Outputs <br>
					<img src="images/1_4 (28).jpg">
				</section>
			</section>



			<section>
				<h3>- Questions? -</h3>
				<br>
				m.amintoosi @ gmail.com
				<br>
				<p>webpage : <a href="http://mamintoosi.ir/">http://mamintoosi.ir</a></p>
				<p>webpage in github : <a href="https://mamintoosi.github.io/">http://mamintoosi.github.io</a></p>
				<p>github : <a href="https://github.com/mamintoosi">mamintoosi</a></p>
				<hr>
				<!-- <small>
 <p> مطالب و سبک اسلاید برگرفته از 
	<a href="https://github.com/mdda/deep-learning-workshop"> Martin Andrews </a>
	و
	<a href="https://hameds.github.io/slides/"> سعیدی‌فرد </a>
 </p>
 </small>
-->
			</section>


		</div>

	</div>

	<script src="../../js/reveal.js"></script>

	<script>

		// More info https://github.com/hakimel/reveal.js#configuration
		Reveal.initialize({
			controls: true,
			progress: true,
			center: true,
			hash: true,
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// More info https://github.com/hakimel/reveal.js#dependencies
			dependencies: [
				{ src: '../../plugin/markdown/marked.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: '../../plugin/markdown/markdown.js', condition: function () { return !!document.querySelector('[data-markdown]'); } },
				{ src: '../../plugin/highlight/highlight.js', async: true },
				{ src: '../../plugin/search/search.js', async: true },
				{ src: '../../plugin/zoom-js/zoom.js', async: true },
				{ src: '../../plugin/notes/notes.js', async: true }
			]
		});

	</script>

</body>

</html>